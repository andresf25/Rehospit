---
title: "R Notebook"
output: html_notebook
---

```{r, message = FALSE, echo = FALSE, eval =TRUE, warning=FALSE}
rm(list = ls())
library(pROC)
library(caret)
library(rpart)
library(randomForest)
library(e1071)
library(purrr)
library(dplyr)
load("data_rehosp.rda")
```

Seleccionando variables del WOE:

```{r, message = FALSE, echo = FALSE, eval =TRUE, warning=FALSE}
data_rehosp %>%
  select(-marcas,
         -id,
         -ramo,
         -est_civil,
         -dias_uci,
         -dias_uce) -> model_rehosp

```

Dividiendo la información en validación (70) y prueba (30)

```{r, message = FALSE, echo = FALSE, eval =TRUE, warning=FALSE}
set.seed(1234)
index <- createDataPartition(model_rehosp$rehosp_oms, p = 0.70, list = FALSE)
trainTransformed <- model_rehosp[ index,]
testTransformed<- model_rehosp[-index,]
```

Después de particionarse los datos se realizan algunas transformaciones para estructurar el formato, en algo aceptable para el paquete empleado; en ese sentido CARET para problemas de clasificación la variable de salida esté en factores y no permite que los niveles de las clases sean "0" o "1".

```{r, message = FALSE, echo = FALSE, eval =TRUE, warning=FALSE}

trainTransformed$rehosp_oms <- relevel(trainTransformed$rehosp_oms)
testTransformed$rehosp_oms <- relevel(testTransformed$rehosp_oms)

levels(trainTransformed$rehosp_oms) <- c("si", "no")
levels(testTransformed$rehosp_oms) <- c("si", "no")

```

Inicialmente se utilizará una máquina de aumento de gradiente (gbm), ya que puede manejar fácilmente las posibles interacciones y no linealidades que se han simulado anteriormente. Los hiperparámetros del modelo se ajustan mediante validación cruzada repetida en el conjunto de entrenamiento, repitiendo cinco veces con diez subconjuntos utilizados en cada repetición.

```{r, message = FALSE, echo = FALSE, eval =TRUE, warning=FALSE}
ctrl <- trainControl(method = "repeatedcv",
                     number = 10,
                     repeats = 5,
                     summaryFunction = twoClassSummary,
                     classProbs = TRUE)

set.seed(5627)

orig_fit <- train(rehosp_oms ~ .,
                  data = trainTransformed,
                  method = "gbm",
                  verbose = FALSE,
                  metric = "sens",
                  preProc = c("center", "scale"),
                  trControl = ctrl)
```

A continuación, se realizarán las predicciones fuera de muestra y evaluando rendimiento a través de la matriz de confusión:

```{r}
predict_ori <- predict(orig_fit, 
                       newdata = testTransformed)

confusionMatrix(reference = testTransformed$rehosp_oms, predict_ori)
```

El desequilibrio en los datos, hace que el modelo sólo pueda predecir bien la clase minoritaria, por tanto, se empleará SMOTE como técnica de rebalanceo para incrementar con valores sintéticos el número de eventos raros (rehospitalizaciones).

```{r, message = FALSE, echo = FALSE, eval =TRUE, warning=FALSE}
set.seed(9560)
trainTransformed <- SMOTE(rehosp_oms ~ ., as.data.frame(trainTransformed), perc.over = 350, perc.under = 150)
```

El resultado el anterior se comparará contra el Gradient Boosting balanceado, un Árbol de Decisión, un Random Forest, una Máquina de Soporte Vectorial Radial y una Máquina de Soporte Vertorial Polynomial, los cuales, al igual que el GBM utilizan un límite de decisión no lineal. 

Tal como se hizo que en el caso anterior, para evaluar el poder predictivo del modelo se hará uso de la sensibilidad como métrica, la cual otorga el número de valores positivos predichos correctamente de todos los valores positivos, es decir, del total de casos de rehospitalización el porcentaje que efectivamente el modelo logra capturar como rehospitalizados; esta métrica, por tanto, cuantifica el total de falsos negativos (personas que se rehospitalizaron identificados incorrectamente como si no lo hubieran hecho).

```{r, message = FALSE, echo = FALSE, eval =TRUE, warning=FALSE}
# set.seed(5627)
# smote_fit <- train(rehosp_oms ~ .,
#                    data = trainTransformed,
#                    method = "gbm",
#                    verbose = FALSE,
#                    metric = "sens",
#                    preProc = c("center", "scale"),
#                    trControl = ctrl)
smote_fit <- readRDS("./smote_fit.rds")

# set.seed(5627)
# model_rf <- train(rehosp_oms ~.,
#                   data = trainTransformed,
#                   method = "rf",
#                   metric = "sens",
#                   preProc = c("center", "scale"),
#                   trControl = ctrl)
model_rf <- readRDS("./model_rf.rds")

# set.seed(5627)
# model_svm <- train(rehosp_oms ~.,
#                   data = trainTransformed,
#                   method = "svmRadial",
#                   metric = "sens",
#                   preProc = c("center", "scale"),
#                   trControl = ctrl)
model_svm <- readRDS("./model_svm.rds")

# set.seed(5627)
# model_svmpol <- train(rehosp_oms ~.,
#                   data = trainTransformed,
#                   method = "svmPoly",
#                   metric = "sens",
#                   preProc = c("center", "scale"),
#                   trControl = ctrl)
model_svmpol <- readRDS("./model_svmpol.rds")

# set.seed(5627)
# model_dt <- train(rehosp_oms ~.,
#                   data = trainTransformed,
#                   method = "rpart",
#                   metric = "sens",
#                   preProc = c("center", "scale"),
#                   trControl = ctrl)
model_dt <- readRDS("./model_dt.rds")

# save the model to disk
# saveRDS(smote_fit, "./smote_fit.rds")
# saveRDS(model_rf, "./model_rf.rds")
# saveRDS(model_svm, "./model_svm.rds")
# saveRDS(model_svmpol, "./model_svmpol.rds")
# saveRDS(model_dt, "./model_dt.rds")
```

Para obtener estadísticas sobre sus diferencia de rendimiento se recopilan los resultados de la validación cruzada con la función resamples

```{r}
model_list <- list(GBM = orig_fit,
                   GBM_SMOTE = smote_fit,
                   Random = model_rf,
                   SVMRadial = model_svm,
                   SVMPoly = model_svmpol,
                   Tree = model_dt)

result_cv <- resamples(list(model_list))

summary(result_cv)
```

Observando las distribuciones obtenidas con el remuestreo:

```{r}
bwplot(resamps, layout = c(3, 2))
```

Dado que los modelos se ajustan a las mismas versiones de los datos de entrenamiento, tiene sentido hacer inferencias sobre las diferencias entre los modelos. De esta manera, se puede llegar a calcular incluso, no sólo las diferencias sino también, usar una prueba t para evaluar la hipótesis nula de que no hay diferencia entre los modelos.

```{r}
difValues <- diff(resamps)
summary(difValues)
```

Podemos examinar la curva ROC real para tener una mejor idea de dónde los modelos ponderados y de muestreo están superando o no, al modelo original en una variedad de umbrales de clasificación.

Una de las ventajas de la curva ROC no depende de la distribución de clase, lo que lo hace útil para evaluar clasificadores que predicen eventos raros como enfermedades o desastres. Por el contrario, la evaluación del rendimiento utilizando la precisión (TP + TN) / (TP + TN + FN + FP) favorecería a los clasificadores que siempre predicen un resultado negativo para la clase minoritaria.

```{r, message = FALSE, echo = FALSE, eval =TRUE, warning=FALSE}
results_list_roc <- list(NA)
num_mod <- 1

for(the_roc in model_list_roc){
  
  results_list_roc[[num_mod]] <- 
    data.frame(TPR = the_roc$sensitivities,
               FPR = 1 - the_roc$specificities,
               Modelos = names(model_list)[num_mod])
  
  num_mod <- num_mod + 1
  
}

results_df_roc <- bind_rows(results_list_roc)

# Curva de ROC para los 4 modelos
custom_col <- c("#7FFFD4", "#32CD32", "#00BFFF", "#008B8B", "#00008B", "#4169E1")

ggplot(aes(x = FPR,  
           y = TPR, 
           group = Modelos), 
       data = results_df_roc) +
  geom_line(aes(color = Modelos), size = 1) +
  scale_color_manual(values = custom_col) +
  geom_abline(intercept = 0, slope = 1, color = "gray", size = 1) +
  theme_minimal()+
  ggtitle("Curvas de ROC")+
  theme(plot.title = element_text(hjust = 0.5))

```

Aquí, vemos que el modelo original parece dominar a los demás, el cual encuentra entre una tasa de falsos positivos entre el 0% y el 70%.

Después de obtenido el mejor modelo, vamos a realizar un ajuste en los hiperparámetros empleando Grid para identificar la combinación más optima. En lugar de especificar los valores exactos para cada parámetro de ajuste, podemos pedirle que utilice cualquier número de valores posibles para cada parámetro de ajuste a través de tuneLength, que se fijará en 10.

```{r}

set.seed(5627)

orig_opt <- train(rehosp_oms ~ .,
                  data = trainTransformed,
                  method = "gbm",
                  verbose = FALSE,
                  metric = "ROC",
                  trControl = ctrl, 
                  tuneLength = 10)
print(orig_opt)
```


```{r}
plot(orig_opt)
```

```{r}
confusionMatrix(reference = testTransformed$rehosp_oms, predict(orig_fit, newdata = testTransformed), positive = "si")
```

```{r}

confusionMatrix(reference = testTransformed$rehosp_oms, predict(model_dt, newdata = testTransformed), positive = "si")

# smote_fit,
#                    Random = model_rf,
#                    SVMRadial = model_svm,
#                    SVMPoly = model_svmpol,
#                    Tree = model_dt)
```
 
```{r}
roc_p <- roc(response = testTransformed$rehosp_oms,
             predictor = predict(orig_fit, newdata = testTransformed, type = "prob")[, "si"], 
             levels= rev(levels(testTransformed$rehosp_oms)))

predicted = predict(orig_fit, newdata = testTransformed, type = "prob")[, "si"]

predicted <- as.data.frame(testTransformed$rehosp_oms, c(predicted))

class(predicted)
class(testTransformed$rehosp_oms)

str(predicted)
plot(roc_p, print.thres = "best")
plot_pred_type_distribution (df = predicted, threshold = 0.7)
```
 
 